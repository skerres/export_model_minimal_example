{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-20 11:45:35.389718: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import base64\n",
    "import sys \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "3.8.10 (default, Jun 22 2022, 20:18:18) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model to classify draft beers\n",
    "\n",
    "This file contains all the model information: the training steps, the batch\n",
    "size and the model itself.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_batch_size():\n",
    "    \"\"\"Returns the batch size that will be used by your solution.\n",
    "    It is recommended to change this value.\n",
    "    \"\"\"\n",
    "    return 25\n",
    "\n",
    "def get_epochs():\n",
    "    \"\"\"Returns number of epochs that will be used by your solution.\n",
    "    It is recommended to change this value.\n",
    "    \"\"\"\n",
    "    return 201\n",
    "\n",
    "def solution(input_layer=None):\n",
    "    \"\"\"Returns a compiled model.\n",
    "\n",
    "    This function is expected to return a model to identity the different beers.\n",
    "    The model's outputs are expected to be probabilities for the classes and\n",
    "    and it should be ready for training.\n",
    "    The input layer specifies the shape of the images. The preprocessing\n",
    "    applied to the images is specified in data.py.\n",
    "\n",
    "    Add your solution below.\n",
    "\n",
    "    Parameters:\n",
    "        input_layer: A tf.keras.layers.InputLayer() specifying the shape of the input.\n",
    "            RGB colored images, shape: (width, height, 3)\n",
    "    Returns:\n",
    "        model: A compiled model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    model = models.Sequential()\n",
    "    # data augmentation\n",
    "    model.add(layers.RandomFlip('horizontal'))\n",
    "    model.add(layers.RandomRotation(0.25))\n",
    "    model.add(layers.RandomZoom(0.25))\n",
    "    # conv layers\n",
    "    # use dropout for regularization\n",
    "    model.add(layers.Conv2D(16, (5, 5), strides=2,\n",
    "                            activation='relu', input_shape=(160, 160, 6)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Conv2D(32, (5, 5), strides=2, activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    # fcnn\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    # use softmax as model is predicting classes\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    # model.compile(optimizer='adam',\n",
    "    #           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    #           metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "class CropImageLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        keras Layer which crops and resizes input images to the highest \n",
    "        ranking bottle detection\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        init of super class\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def call(self, input, boxes, scores, classes):\n",
    "        \"\"\"\n",
    "        Crops and resizes input images such that they concat the bottle detection and the glass detection\n",
    "            with the highest detection score\n",
    "        Parameters:\n",
    "            input: input tensor of shape (batch_size, width, height, 3)\n",
    "            boxes: np.array of shape (batch_size, classification_per_image, 4) containing\n",
    "                coordinates of detection boxes\n",
    "            classes: np.array of shape (batch_size, classification_per_image) containig\n",
    "                class ids of the classifications\n",
    "        Returns:\n",
    "            tensor of shape (batch_size, width, height, 6)\n",
    "            images respectively cropped and resized to the best bottle and glass detection box\n",
    "        \"\"\"\n",
    "        boxes_filtered = []\n",
    "        box_ind = []\n",
    "        BOTTLE_ID = 44 # bottle id for object detection\n",
    "        GLASS_ID = 46 # bottle id for object detection\n",
    "        bottle_boxes_filtered, glass_boxes_filtered = [], []\n",
    "        bottle_box_ind, glass_box_ind = [], []\n",
    "        tf.print(\"=\"*80)\n",
    "        tf.print(\"boxes: \" + str(boxes))\n",
    "        tf.print(\"boxes.shape: \" + str(boxes.shape))\n",
    "        # During model creation boxes.shape[0] is None as model gets created\n",
    "        # Return therefore just concatenated input\n",
    "        if not boxes.shape[0]:\n",
    "            return tf.concat([input, input], axis = 3)\n",
    "\n",
    "        for i in range(boxes.shape[0]): #batch\n",
    "            for j in range(boxes.shape[1]): # detections\n",
    "                # if classes[i][j] == BOTTLE_ID:\n",
    "                    # print(\"(i,j):(\" + str(i) + \",\" + str(j) + \")\" + \" score: \" + str(scores[i][j].numpy()) + \" class: \" + str(classes[i][j].numpy()))\n",
    "                if classes[i][j] == BOTTLE_ID and scores[i][j] > 0.5:\n",
    "                    bottle_box = boxes[i][j].numpy() / 159\n",
    "                    break\n",
    "            # do not crop if no bottle was detected\n",
    "            if j == boxes.shape[1] - 1:\n",
    "                bottle_box = np.array([0, 0, 1, 1])\n",
    "            bottle_box_ind.append(i)\n",
    "            bottle_boxes_filtered.append(bottle_box)\n",
    "\n",
    "        for i in range(boxes.shape[0]): #batch\n",
    "            for j in range(boxes.shape[1]): # detections\n",
    "                # if classes[i][j] == GLASS_ID:\n",
    "                    # print(\"(i,j):(\" + str(i) + \",\" + str(j) + \")\" + \" score: \" + str(scores[i][j].numpy()) + \" class: \" + str(classes[i][j].numpy()))\n",
    "                if classes[i][j] == GLASS_ID and scores[i][j] > 0.5:\n",
    "                    glass_box = boxes[i][j].numpy() / 159\n",
    "                    break\n",
    "            # do not crop if no bottle was detected\n",
    "            if j == boxes.shape[1] - 1:\n",
    "                glass_box = np.array([0, 0, 1, 1])\n",
    "            glass_box_ind.append(i)\n",
    "            glass_boxes_filtered.append(glass_box)\n",
    "\n",
    "        batch_glass_cropped = tf.image.crop_and_resize(input, glass_boxes_filtered, glass_box_ind, (160, 160))\n",
    "        batch_bottle_cropped = tf.image.crop_and_resize(input, bottle_boxes_filtered, bottle_box_ind, (160, 160))\n",
    "        batch_data_cropped = tf.concat([batch_glass_cropped, batch_bottle_cropped], axis = 3)\n",
    "        return batch_data_cropped\n",
    "\n",
    "\n",
    "class ComposedModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Model composed of a pretrained object detector and a prediction head trained on the image dataset\n",
    "    prediction head loaded from saved model, was trained in a previous step\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ComposedModel, self).__init__()\n",
    "        # path to efficientdet model on tf hub\n",
    "        model_handle = \"https://tfhub.dev/tensorflow/efficientdet/lite3/detection/1\"\n",
    "        # path of prediction head model weights\n",
    "        prediction_head_path = \"prediction_head_model\"\n",
    "        self.efficientdet = hub.KerasLayer(model_handle, trainable=False)\n",
    "        self.crop_image_layer = CropImageLayer()\n",
    "        self.prediction_head = tf.saved_model.load(prediction_head_path)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "            Compute classification scores for the input batch\n",
    "            Parameters:\n",
    "                inputs: input tensor to be classified of shape (batch_size, height, width, 3)\n",
    "            Return: softmax predictions of shape (batch_size, number_of_classes)\n",
    "        \"\"\"\n",
    "        # convert to uint8 from float32 as efficientdet expects image pixels in [0,256]\n",
    "        inputs_uint8 = tf.image.convert_image_dtype(inputs, dtype=tf.uint8, saturate=False)\n",
    "        # perform object detection on input to obtain boxes and classes of classification candidates\n",
    "        boxes, scores, classes, _ = self.efficientdet(inputs_uint8)\n",
    "        # crop inputs respectively to bottles with highest classification score\n",
    "        inputs_cropped = self.crop_image_layer(inputs, boxes, scores, classes)\n",
    "        # compute predictions for respective image classes\n",
    "        result = self.prediction_head(inputs_cropped)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_exported_model():\n",
    "    \"\"\"\n",
    "    Compare the model directly after training to the exported model.\n",
    "\n",
    "    Even though the models receive the same image (just in different formats) the predictions differ.\n",
    "    The error therefore probably comes from the export of the model\n",
    "\n",
    "    # \"\"\"\n",
    "    # create composed model, the solution to the coding challenge\n",
    "    ml_model = ComposedModel()\n",
    "    ml_model.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy'])\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    basename = \"composed_model_\" + time_str\n",
    "    # export to \"output/composed_model\"\n",
    "    # ml_model_reloaded = tf.saved_model.load(os.path.join(\"output\", basename))\n",
    "    # open test image\n",
    "    with open(\"0chimayblue_000.jpg\", \"rb\") as img_file:\n",
    "        # open test image encoded in base64\n",
    "        img_base64 = base64.b64encode(img_file.read(), altchars=str.encode(\"-_\"))\n",
    "    # create image from base64 encoded string\n",
    "    img = Image.open(BytesIO(base64.b64decode(img_base64, altchars=str.encode(\"-_\"))))\n",
    "    # from [0, 255] -> [0, 1]\n",
    "    img = np.array(img) / 255.\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    # convert base64 encoded image to string tensor\n",
    "    predictions = ml_model(img, training=False)  # predictions for trained model\n",
    "    ml_model.save(os.path.join(\"output\", basename), save_format = \"tf\")\n",
    "    # reload model which has just been exported\n",
    "    ml_model_reloaded = tf.keras.models.load_model(os.path.join(\"output\", basename))\n",
    "    ml_model_reloaded.compile(optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy'])\n",
    "    tf.print(ml_model_reloaded)\n",
    "    predictions_reloaded = ml_model_reloaded(img, training=False)  # predictions for trained model which was exported and then reimported\n",
    "    print(\"trained model: \" + str(predictions.numpy())) #output: [[9.7456181e-01 2.0527570e-04 1.9158632e-02 1.0962408e-03 4.9780323e-03]]\n",
    "    print(\"served model: \" + str(predictions_reloaded.numpy())) #output:  [[0.00636891 0.9207034  0.00868604 0.00923812 0.05500359]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-20 11:45:37.756213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-20 11:45:37.810439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/skerres/.local/lib/python3.8/site-packages/cv2/../../lib64:/usr/include:/usr/local/lib:/usr/local/cuda-10.1/targets/x86_64-linux/lib:/usr/local/cuda-11.0/targets/x86_64-linux/lib\n",
      "2022-08-20 11:45:37.810456: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-08-20 11:45:37.811007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "boxes: tf.Tensor(\n",
      "[[[3.94008160e-01 4.05479813e+01 6.24505920e+01 6.73040085e+01]\n",
      "  [6.03947639e-01 7.00556107e+01 6.71987610e+01 9.62328339e+01]\n",
      "  [3.17409039e-01 1.47847090e+01 5.69000549e+01 4.08496933e+01]\n",
      "  [1.14425850e+01 2.96565056e+00 1.59142944e+02 1.59820374e+02]\n",
      "  [3.56609001e+01 1.21625359e+02 6.99773331e+01 1.58188080e+02]\n",
      "  [1.61480665e+00 9.51249619e+01 4.69383240e+01 1.59178757e+02]\n",
      "  [8.04954767e-02 3.00335884e-01 1.57853222e+01 1.64636307e+01]\n",
      "  [0.00000000e+00 7.17967758e+01 1.76246948e+01 1.16832848e+02]\n",
      "  [6.02782898e+01 6.11905813e+00 8.56941223e+01 5.66452026e+01]\n",
      "  [0.00000000e+00 5.93407364e+01 1.36608496e+01 8.56794281e+01]\n",
      "  [1.16609512e+02 1.01068100e+02 1.59503967e+02 1.58759552e+02]\n",
      "  [7.72538300e+01 1.35561127e+02 9.49364395e+01 1.57361465e+02]\n",
      "  [0.00000000e+00 1.22470840e+02 1.52644730e+01 1.39754181e+02]\n",
      "  [7.59778290e+01 7.99659958e+01 1.05999672e+02 1.04677963e+02]\n",
      "  [0.00000000e+00 3.32862968e+01 1.86418324e+01 5.09915161e+01]\n",
      "  [7.65657043e+01 1.02871216e+02 1.27262131e+02 1.58160049e+02]\n",
      "  [2.71315575e-01 8.94597626e+01 1.71791286e+01 1.17296005e+02]\n",
      "  [7.33368874e+00 1.37216934e+02 4.49124985e+01 1.59414734e+02]\n",
      "  [9.02307034e-01 4.04653320e+01 6.31887436e+01 6.68303604e+01]\n",
      "  [3.68855000e-01 1.55190105e+01 5.69517822e+01 4.05776291e+01]\n",
      "  [5.81378937e-01 7.01586456e+01 6.78040237e+01 9.54947357e+01]\n",
      "  [5.62344780e+01 6.60711288e+00 1.58641373e+02 1.24668854e+02]\n",
      "  [7.60258026e+01 8.89266663e+01 9.00121613e+01 1.02200539e+02]\n",
      "  [0.00000000e+00 1.11767105e+02 1.61485062e+01 1.28920807e+02]\n",
      "  [7.97425156e+01 1.08469040e+02 9.68614807e+01 1.37172287e+02]\n",
      "  [1.50490999e-01 1.42451344e+01 2.10508194e+01 2.39713383e+01]\n",
      "  [2.37896442e-01 1.29900116e+02 1.47880564e+01 1.59109665e+02]\n",
      "  [9.47064819e+01 1.37570618e+02 1.14291496e+02 1.59856888e+02]\n",
      "  [6.18133965e+01 5.57748079e-01 1.59456055e+02 2.94863548e+01]\n",
      "  [7.34529800e+01 4.94931335e+01 1.29992264e+02 1.07310455e+02]\n",
      "  [3.87548208e-01 9.30739975e+01 3.34892044e+01 1.45131805e+02]\n",
      "  [9.70071888e+00 6.72736359e+01 2.35486507e+01 8.08751602e+01]\n",
      "  [3.35575104e+01 1.25379875e+02 7.04123993e+01 1.59346344e+02]\n",
      "  [0.00000000e+00 1.02570677e+01 1.89808655e+01 5.15108261e+01]\n",
      "  [1.30794644e+00 1.73053369e-01 3.05965137e+01 1.62683845e+00]\n",
      "  [2.44899988e-01 7.58432150e-01 2.06818733e+01 2.16727791e+01]\n",
      "  [9.86852837e+00 1.37726578e+02 2.80565166e+01 1.59399048e+02]\n",
      "  [3.37268591e+00 6.50981750e+01 5.04814148e+01 8.01689148e+01]\n",
      "  [9.31101151e+01 4.11594696e+01 1.10180290e+02 7.16448135e+01]\n",
      "  [3.00986366e+01 1.53685791e+02 4.49414635e+01 1.59543900e+02]\n",
      "  [1.06645703e+00 5.62274628e+01 3.39039764e+01 6.86025162e+01]\n",
      "  [3.19950581e+00 6.21056557e-01 1.58308594e+02 1.58471359e+02]\n",
      "  [8.85750732e+01 8.15274658e+01 1.05283218e+02 1.03841896e+02]\n",
      "  [4.44824791e+01 1.49402588e+02 8.40901642e+01 1.59309982e+02]\n",
      "  [0.00000000e+00 9.29505081e+01 1.97665632e+00 1.20685394e+02]\n",
      "  [1.51168108e-01 5.97122993e+01 1.53634434e+01 1.00580429e+02]\n",
      "  [0.00000000e+00 4.64247167e-01 1.53368988e+01 9.79485607e+00]\n",
      "  [7.12891817e-01 3.06059818e+01 3.05104790e+01 4.40029526e+01]\n",
      "  [2.51391506e+01 1.13992851e+02 4.84392853e+01 1.57937332e+02]\n",
      "  [1.36518768e+02 2.19501257e-01 1.59253311e+02 2.27184868e+01]\n",
      "  [2.83252001e-01 6.72330952e+00 1.35937080e+01 1.50762424e+01]\n",
      "  [3.60969543e+01 8.91135254e+01 7.14131241e+01 1.47320496e+02]\n",
      "  [0.00000000e+00 3.64656487e+01 2.24849677e+00 5.22616615e+01]\n",
      "  [1.05180912e+01 2.98306942e-01 6.94653320e+01 1.76266594e+01]\n",
      "  [7.76185455e+01 1.35493347e+02 9.96512756e+01 1.59006210e+02]\n",
      "  [0.00000000e+00 3.19341373e+01 1.74304028e+01 7.91540070e+01]\n",
      "  [2.88177133e-01 9.20809326e+01 7.94941425e+00 1.15618607e+02]\n",
      "  [1.61081486e+01 1.53826202e+02 3.39327545e+01 1.59723358e+02]\n",
      "  [1.28363754e+02 1.52691498e+02 1.40935837e+02 1.59797577e+02]\n",
      "  [6.18067384e-02 1.35213366e+01 2.38590193e+00 2.78897629e+01]\n",
      "  [4.71818447e-02 1.43385681e+02 1.22169104e+01 1.59631393e+02]\n",
      "  [5.61198883e+01 1.26021147e+00 1.58409302e+02 5.45157700e+01]\n",
      "  [9.37386751e-01 8.03583450e+01 2.55973015e+01 9.42683716e+01]\n",
      "  [8.37682037e+01 0.00000000e+00 9.94139786e+01 4.00382423e+00]\n",
      "  [1.50567894e+01 4.24728298e+00 2.30670624e+01 1.59571400e+01]\n",
      "  [2.73342133e-01 9.11267090e+01 2.12085323e+01 1.36910080e+02]\n",
      "  [1.62390172e-01 6.62570572e+01 9.56997395e+00 8.32203522e+01]\n",
      "  [9.20177612e+01 1.57463551e-01 1.40641174e+02 1.35780411e+01]\n",
      "  [2.89183140e+01 1.28962692e+02 3.92194901e+01 1.56623962e+02]\n",
      "  [1.14699516e+02 1.03023788e+02 1.58047729e+02 1.60000000e+02]\n",
      "  [0.00000000e+00 3.47991943e+00 6.31635475e+01 1.46318115e+02]\n",
      "  [8.54822397e-01 1.81593895e-01 3.83891335e+01 3.94488478e+00]\n",
      "  [9.65890884e-01 3.26025696e+01 4.89641418e+01 5.16391754e+01]\n",
      "  [4.19437438e-01 6.88460846e+01 2.86906123e+00 8.60369720e+01]\n",
      "  [5.42968512e-01 1.54782654e+02 1.30655375e+01 1.59625580e+02]\n",
      "  [5.91789780e+01 4.82125998e+00 8.76414490e+01 5.66496353e+01]\n",
      "  [4.32762184e+01 1.57028061e+02 7.38450089e+01 1.59387726e+02]\n",
      "  [3.17353916e+01 1.55777374e+02 5.35047989e+01 1.59207932e+02]\n",
      "  [1.87447071e-01 1.91946924e+00 2.65427589e+00 2.00273514e+01]\n",
      "  [1.53638821e+01 1.47175690e+02 2.22190113e+01 1.59743668e+02]\n",
      "  [6.63176179e-01 1.20656548e+02 1.53995209e+01 1.27460846e+02]\n",
      "  [9.51413803e+01 4.14456940e+01 1.16164101e+02 7.99995880e+01]\n",
      "  [1.11259651e+02 3.23920021e+01 1.59944916e+02 1.32348251e+02]\n",
      "  [2.65172844e+01 1.04967865e+02 4.73738480e+01 1.20862137e+02]\n",
      "  [6.92402496e+01 1.50241562e+02 9.65555725e+01 1.59477692e+02]\n",
      "  [2.01900005e-01 1.15821648e+02 1.70688267e+01 1.48039856e+02]\n",
      "  [1.61373615e-01 1.36866474e+01 7.52726650e+00 2.50967007e+01]\n",
      "  [1.05934143e+00 1.97398439e-01 2.08283997e+01 1.97564840e+00]\n",
      "  [1.27055435e+02 1.49747635e+02 1.59300705e+02 1.59787476e+02]\n",
      "  [1.38054132e+01 9.56657562e+01 3.20147781e+01 1.38904434e+02]\n",
      "  [7.65583515e-01 4.80312653e+01 2.44611816e+01 5.82168655e+01]\n",
      "  [0.00000000e+00 2.67952309e+01 2.31190014e+01 4.10759506e+01]\n",
      "  [1.46677261e+02 1.48823857e-01 1.59736282e+02 1.21598511e+01]\n",
      "  [1.02988586e+01 6.84374390e+01 2.07304096e+01 8.05795898e+01]\n",
      "  [8.96328125e+01 1.36465561e+02 1.13092941e+02 1.59648407e+02]\n",
      "  [1.01165056e+01 1.40886810e+02 2.03296776e+01 1.59693100e+02]\n",
      "  [3.38843651e+01 1.47265854e+02 7.98607254e+01 1.59576736e+02]\n",
      "  [5.58020210e+00 6.17640343e+01 4.52215614e+01 7.25564651e+01]\n",
      "  [6.28376102e+00 9.83677597e+01 1.64723148e+01 1.09213722e+02]\n",
      "  [5.24477730e+01 9.82576141e+01 1.43205841e+02 1.59068436e+02]]], shape=(1, 100, 4), dtype=float32)\n",
      "boxes.shape: (1, 100, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1048). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/composed_model_2022-08-20-11-45-49/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/composed_model_2022-08-20-11-45-49/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.saving.saved_model.load.ComposedModel object at 0x7f6d7180a460>\n",
      "================================================================================\n",
      "boxes: Tensor(\"keras_layer/StatefulPartitionedCall:0\", shape=(None, 100, 4), dtype=float32)\n",
      "boxes.shape: (None, 100, 4)\n",
      "trained model: [[9.7456181e-01 2.0527570e-04 1.9158632e-02 1.0962408e-03 4.9780323e-03]]\n",
      "served model: [[0.00636891 0.9207034  0.00868604 0.00923812 0.05500359]]\n"
     ]
    }
   ],
   "source": [
    "tf_logger = logging.getLogger(\"tensorflow\")\n",
    "tf_logger.setLevel(logging.INFO)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(tf_logger.level // 10)\n",
    "# run test\n",
    "test_exported_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
